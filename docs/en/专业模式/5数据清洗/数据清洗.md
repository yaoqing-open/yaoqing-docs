# Introduction to Data Transformation

## Functional Overview
Data transformation provides data engineers and developers with an efficient, professional, and intelligent data development platform. By providing capabilities such as script development, visual development, task orchestration, task publishing, and task operations and maintenance, it helps organizations and businesses efficiently build real-time data lakehouses.


## Feature Details
### Modeling
- Visual Mode (Recommended): You can create a new transformation model table through a graphical interface by navigating to: Data Source -> Output Source -> Transformation Warehouse -> ETL Layer -> New Table.
- DDL Mode: You can create a new table using SQL statements by navigating to: Data Source -> Output Source -> Transformation Warehouse -> ETL Layer -> Query.

### Layer Concept
- All source database data is synchronized to the `input` layer of the data warehouse. All transformation tables are created in the `etl` transformation layer of the data warehouse.
- A task's level is determined by the maximum level of the transformation tasks that produce its input tables.
- If all input tables are from the `input` layer, the current task is Level 1.
- If the input tables include an output table from a Level *n* transformation task, the current task becomes Level *n+1*.
- Task levels ensure clear data transformation dependencies, enabling **layered transformation and streamed triggering** while preventing circular dependencies.

### Scripting Guide
1. To reduce code duplication, the platform supports using global variables `${var}` in SQL to replace repetitive code.
2. For a list of supported SQL functions, refer to the documentation: [👉 Yaoqing SQL](爻擎%20SQL.md)
3. For SQL transformation standards, see: [👉 Transformation Guidelines](清洗规范.md)
4. For SQL editor shortcuts, see: [👉 Keyboard Shortcuts](快捷键.md)

### Job Specification
- To prevent the real-time state from growing indefinitely, the platform processes real-time data cleansing and transformation based on user-provided SQL. Users are required to specify a time field in a `WHERE` clause to limit the real-time computation scope。
- Upon initial startup, the job will read and process data according to the `WHERE` clause. A one-time full backfill of the historical data (within the defined scope) will be performed automatically.
- When a job is restarted after being stopped or encountering an error, it will, by default, resume incremental computation from the last checkpoint. If recovery from the checkpoint is not possible, the job will perform a full data backfill before resuming its operation.
- The platform provides two distinct execution environments tailored for small-scale and large-scale jobs, respectively, and will intelligently switch between them based on the job's status and workload.


